#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
3ë‹¨ê³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ
1ë‹¨ê³„: feedparserë¡œ RSS íŒŒì‹±
2ë‹¨ê³„: Seleniumìœ¼ë¡œ Google News ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬
3ë‹¨ê³„: newspaperë¡œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
"""

import logging
import feedparser
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
import time
from dataclasses import dataclass
from typing import List, Optional

# newspaper ì„í¬íŠ¸ í™•ì¸
try:
    from newspaper import Article

    NEWSPAPER_AVAILABLE = True
    print("âœ… newspaper ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    NEWSPAPER_AVAILABLE = False
    print(f"âŒ newspaper ì‚¬ìš© ë¶ˆê°€: {e}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class NewsItem:
    """ë‰´ìŠ¤ ì•„ì´í…œ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    google_news_url: str
    source: str
    published_at: str
    real_url: Optional[str] = None
    content: Optional[str] = None
    authors: Optional[List[str]] = None
    keywords: Optional[List[str]] = None


class Step1_RSSParser:
    """1ë‹¨ê³„: RSS í”¼ë“œ íŒŒì‹±"""

    BASE_URL = "https://news.google.com/rss"
    KR_PARAMS = "hl=ko&gl=KR&ceid=KR:ko"

    def parse_rss_feed(self, rss_url: str) -> List[NewsItem]:
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ë§í¬ë“¤ ì¶”ì¶œ"""
        logger.info(f"ğŸ“¡ 1ë‹¨ê³„: RSS í”¼ë“œ íŒŒì‹± - {rss_url}")

        try:
            # feedparserë¡œ RSS íŒŒì‹±
            feed = feedparser.parse(rss_url)

            if not feed.entries:
                logger.warning("RSS í”¼ë“œì—ì„œ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []

            news_items = []

            for entry in feed.entries:
                try:
                    # RSSì—ì„œ ê¸°ë³¸ ì •ë³´ë§Œ ì¶”ì¶œ (ì•„ì§ ì‹¤ì œ URLì´ë‚˜ ë‚´ìš©ì€ ì—†ìŒ)
                    news_item = NewsItem(
                        title=entry.title,
                        google_news_url=entry.link,  # ì´ê²ƒì€ Google News ë§í¬
                        source=getattr(entry, 'source', {}).get('title', 'Unknown'),
                        published_at=getattr(entry, 'published', 'Unknown')
                    )

                    news_items.append(news_item)

                except Exception as e:
                    logger.error(f"RSS í•­ëª© ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue

            logger.info(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: {len(news_items)}ê°œ ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘")
            return news_items

        except Exception as e:
            logger.error(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []

    def get_realtime_news_links(self, max_items: int = 10) -> List[NewsItem]:
        """ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë§í¬ë“¤ ê°€ì ¸ì˜¤ê¸°"""
        main_rss_url = f"{self.BASE_URL}?{self.KR_PARAMS}"
        news_items = self.parse_rss_feed(main_rss_url)
        return news_items[:max_items]

    def get_category_news_links(self, category: str, max_items: int = 10) -> List[NewsItem]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ë§í¬ë“¤ ê°€ì ¸ì˜¤ê¸°"""
        category_urls = {
            'business': f"{self.BASE_URL}/headlines/section/topic/BUSINESS?{self.KR_PARAMS}",
            'technology': f"{self.BASE_URL}/headlines/section/topic/TECHNOLOGY?{self.KR_PARAMS}",
            'politics': f"{self.BASE_URL}/headlines/section/topic/POLITICS?{self.KR_PARAMS}",
            'sports': f"{self.BASE_URL}/headlines/section/topic/SPORTS?{self.KR_PARAMS}"
        }

        if category not in category_urls:
            logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category}")
            return []

        news_items = self.parse_rss_feed(category_urls[category])
        return news_items[:max_items]


class Step2_URLResolver:
    """2ë‹¨ê³„: Google News ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬"""

    def __init__(self):
        self.driver = None
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def setup_selenium(self):
        """Selenium WebDriver ì„¤ì •"""
        if self.driver is None:
            try:
                chrome_options = Options()
                chrome_options.add_argument("--headless")
                chrome_options.add_argument("--no-sandbox")
                chrome_options.add_argument("--disable-dev-shm-usage")

                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                logger.info("âœ… Selenium WebDriver ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                logger.error(f"Selenium ì„¤ì • ì˜¤ë¥˜: {e}")
                self.driver = None

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            self.driver.quit()
            self.driver = None

    def resolve_google_news_url(self, google_news_url: str) -> str:
        """Google News URLì„ ì‹¤ì œ ì–¸ë¡ ì‚¬ URLë¡œ ë³€í™˜"""
        logger.info(f"ğŸ”„ 2ë‹¨ê³„: URL ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬")

        try:
            # Google News ë§í¬ê°€ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if 'news.google.com' not in google_news_url:
                return google_news_url

            # Seleniumìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬
            if not self.driver:
                self.setup_selenium()

            if not self.driver:
                logger.error("Selenium ì„¤ì • ì‹¤íŒ¨")
                return google_news_url

            logger.info(f"Google News ë§í¬ ì²˜ë¦¬: {google_news_url[:80]}...")
            self.driver.get(google_news_url)

            # ë¦¬ë‹¤ì´ë ‰íŠ¸ ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
            WebDriverWait(self.driver, 10).until(
                lambda driver: 'news.google.com' not in driver.current_url
            )

            real_url = self.driver.current_url

            if 'news.google.com' not in real_url:
                logger.info(f"âœ… ì‹¤ì œ URL ë°œê²¬: {real_url[:80]}...")
                return real_url
            else:
                logger.warning("ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‹¤íŒ¨ - Google Newsì—ì„œ ë²—ì–´ë‚˜ì§€ ëª»í•¨")
                return google_news_url

        except Exception as e:
            logger.error(f"URL ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return google_news_url

    def resolve_news_urls(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """ë‰´ìŠ¤ ì•„ì´í…œë“¤ì˜ URLì„ ì‹¤ì œ ë§í¬ë¡œ ë³€í™˜"""
        logger.info(f"ğŸ”„ 2ë‹¨ê³„: {len(news_items)}ê°œ URL ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬ ì‹œì‘")

        for i, news_item in enumerate(news_items):
            logger.info(f"URL ì²˜ë¦¬ ì¤‘ ({i + 1}/{len(news_items)}): {news_item.title[:50]}...")

            real_url = self.resolve_google_news_url(news_item.google_news_url)
            news_item.real_url = real_url

            time.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ

        logger.info(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: {len(news_items)}ê°œ URL ì²˜ë¦¬ ì™„ë£Œ")
        return news_items


class Step3_ContentExtractor:
    """3ë‹¨ê³„: ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def extract_with_newspaper(self, url: str) -> dict:
        """newspaperë¡œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ"""
        if not NEWSPAPER_AVAILABLE:
            return None

        try:
            logger.info(f"ğŸ“° newspaperë¡œ ë‚´ìš© ì¶”ì¶œ: {url[:50]}...")

            article = Article(url, language='ko')
            article.download()
            article.parse()

            # NLP ì²˜ë¦¬ (í‚¤ì›Œë“œ ì¶”ì¶œ)
            try:
                article.nlp()
            except:
                pass

            content = article.text.strip() if article.text else ""

            if len(content) < 100:
                return None

            return {
                'content': content,
                'authors': list(article.authors) if article.authors else [],
                'keywords': list(article.keywords)[:5] if hasattr(article, 'keywords') else []
            }

        except Exception as e:
            logger.error(f"newspaper ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def extract_with_beautifulsoup(self, url: str) -> dict:
        """BeautifulSoupìœ¼ë¡œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ (ëŒ€ì•ˆ)"""
        try:
            logger.info(f"ğŸ² BeautifulSoupìœ¼ë¡œ ë‚´ìš© ì¶”ì¶œ: {url[:50]}...")

            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ë³¸ë¬¸ ì¶”ì¶œ
            content_selectors = [
                '.article-content', '.entry-content', '.post-content',
                '.content', 'article', '.article-body'
            ]

            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                    for tag in content_elem.find_all(['script', 'style', 'nav', 'footer']):
                        tag.decompose()

                    content = content_elem.get_text().strip()
                    if len(content) > 200:
                        break

            # p íƒœê·¸ë¡œ ëŒ€ì²´
            if len(content) < 200:
                paragraphs = soup.find_all('p')
                content = '\n'.join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 30])

            return {
                'content': content,
                'authors': [],
                'keywords': []
            }

        except Exception as e:
            logger.error(f"BeautifulSoup ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {
                'content': f"ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}",
                'authors': [],
                'keywords': []
            }

    def extract_article_content(self, news_item: NewsItem) -> NewsItem:
        """ë‰´ìŠ¤ ì•„ì´í…œì˜ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ"""
        url = news_item.real_url or news_item.google_news_url

        logger.info(f"ğŸ“„ 3ë‹¨ê³„: ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ - {news_item.title[:50]}...")

        # newspaper ìš°ì„  ì‹œë„
        if NEWSPAPER_AVAILABLE:
            result = self.extract_with_newspaper(url)
            if result and len(result['content']) > 100:
                news_item.content = result['content'][:2000] + "..." if len(result['content']) > 2000 else result[
                    'content']
                news_item.authors = result['authors']
                news_item.keywords = result['keywords']
                return news_item

        # BeautifulSoupìœ¼ë¡œ ëŒ€ì²´
        result = self.extract_with_beautifulsoup(url)
        news_item.content = result['content'][:2000] + "..." if len(result['content']) > 2000 else result['content']
        news_item.authors = result['authors']
        news_item.keywords = result['keywords']

        return news_item

    def extract_contents(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """ë‰´ìŠ¤ ì•„ì´í…œë“¤ì˜ ë‚´ìš© ì¶”ì¶œ"""
        logger.info(f"ğŸ“„ 3ë‹¨ê³„: {len(news_items)}ê°œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹œì‘")

        for i, news_item in enumerate(news_items):
            logger.info(f"ë‚´ìš© ì¶”ì¶œ ì¤‘ ({i + 1}/{len(news_items)})")
            news_items[i] = self.extract_article_content(news_item)
            time.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ

        logger.info(f"âœ… 3ë‹¨ê³„ ì™„ë£Œ: {len(news_items)}ê°œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ")
        return news_items


class GoogleRSSCrawler:
    """í†µí•© Google RSS í¬ë¡¤ëŸ¬ - 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤"""

    def __init__(self):
        self.step1_parser = Step1_RSSParser()
        self.step2_resolver = Step2_URLResolver()
        self.step3_extractor = Step3_ContentExtractor()

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.step2_resolver.cleanup()

    def get_realtime_news(self, max_items: int = 10, extract_content: bool = True) -> List[NewsItem]:
        """ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ (3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤)"""
        logger.info(f"ğŸš€ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_items}ê°œ)")

        # 1ë‹¨ê³„: RSSì—ì„œ ë‰´ìŠ¤ ë§í¬ë“¤ ìˆ˜ì§‘
        news_items = self.step1_parser.get_realtime_news_links(max_items)

        if not news_items:
            logger.error("1ë‹¨ê³„ì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []

        # 2ë‹¨ê³„: Google News ë§í¬ë¥¼ ì‹¤ì œ ë§í¬ë¡œ ë³€í™˜
        news_items = self.step2_resolver.resolve_news_urls(news_items)

        # 3ë‹¨ê³„: ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ (ì˜µì…˜)
        if extract_content:
            news_items = self.step3_extractor.extract_contents(news_items)

        logger.info(f"ğŸ‰ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_items)}ê°œ")
        return news_items

    def get_category_news(self, category: str, max_items: int = 10, extract_content: bool = True) -> List[NewsItem]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ (3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤)"""
        logger.info(f"ğŸš€ {category} ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")

        # 1ë‹¨ê³„: RSSì—ì„œ ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ë§í¬ë“¤ ìˆ˜ì§‘
        news_items = self.step1_parser.get_category_news_links(category, max_items)

        if not news_items:
            logger.error("1ë‹¨ê³„ì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []

        # 2ë‹¨ê³„: Google News ë§í¬ë¥¼ ì‹¤ì œ ë§í¬ë¡œ ë³€í™˜
        news_items = self.step2_resolver.resolve_news_urls(news_items)

        # 3ë‹¨ê³„: ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ (ì˜µì…˜)
        if extract_content:
            news_items = self.step3_extractor.extract_contents(news_items)

        logger.info(f"ğŸ‰ {category} ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_items)}ê°œ")
        return news_items


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
def test_step_by_step():
    """ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸"""
    print("=== 3ë‹¨ê³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")

    # 1ë‹¨ê³„ë§Œ í…ŒìŠ¤íŠ¸
    print("\n--- 1ë‹¨ê³„: RSS íŒŒì‹± ---")
    parser = Step1_RSSParser()
    news_links = parser.get_realtime_news_links(3)

    for i, news in enumerate(news_links, 1):
        print(f"[{i}] {news.title}")
        print(f"    Google News URL: {news.google_news_url[:80]}...")
        print(f"    ì–¸ë¡ ì‚¬: {news.source}")

    # 2ë‹¨ê³„ ì¶”ê°€
    print("\n--- 2ë‹¨ê³„: URL ë¦¬ë‹¤ì´ë ‰íŠ¸ ---")
    resolver = Step2_URLResolver()

    try:
        news_with_real_urls = resolver.resolve_news_urls(news_links[:2])  # 2ê°œë§Œ í…ŒìŠ¤íŠ¸

        for i, news in enumerate(news_with_real_urls, 1):
            print(f"[{i}] {news.title}")
            print(f"    ì‹¤ì œ URL: {news.real_url[:80]}...")

    finally:
        resolver.cleanup()

    # 3ë‹¨ê³„ ì¶”ê°€
    print("\n--- 3ë‹¨ê³„: ë‚´ìš© ì¶”ì¶œ ---")
    extractor = Step3_ContentExtractor()

    news_with_content = extractor.extract_contents(news_with_real_urls[:1])  # 1ê°œë§Œ í…ŒìŠ¤íŠ¸

    for i, news in enumerate(news_with_content, 1):
        print(f"[{i}] {news.title}")
        print(f"    ì €ì: {', '.join(news.authors) if news.authors else 'ì •ë³´ ì—†ìŒ'}")
        print(f"    í‚¤ì›Œë“œ: {', '.join(news.keywords) if news.keywords else 'ì •ë³´ ì—†ìŒ'}")
        print(f"    ë‚´ìš©: {news.content[:200] if news.content else 'ë‚´ìš© ì—†ìŒ'}...")


def test_full_process():
    """ì „ì²´ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print("\n=== ì „ì²´ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸ ===")

    crawler = GoogleRSSCrawler()

    try:
        # ì‹¤ì‹œê°„ ë‰´ìŠ¤ 3ê°œ (ë‚´ìš© ì¶”ì¶œ í¬í•¨)
        news_items = crawler.get_realtime_news(3, extract_content=True)

        print(f"\nìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(news_items)}ê°œ")

        for i, news in enumerate(news_items, 1):
            print(f"\n[{i}] {news.title}")
            print(f"    ì–¸ë¡ ì‚¬: {news.source}")
            print(f"    Google News URL: {news.google_news_url[:50]}...")
            print(f"    ì‹¤ì œ URL: {news.real_url[:50] if news.real_url else 'ì—†ìŒ'}...")
            print(f"    ì €ì: {', '.join(news.authors) if news.authors else 'ì •ë³´ ì—†ìŒ'}")
            print(f"    í‚¤ì›Œë“œ: {', '.join(news.keywords) if news.keywords else 'ì •ë³´ ì—†ìŒ'}")
            print(f"    ë‚´ìš©: {news.content[:150] if news.content else 'ë‚´ìš© ì—†ìŒ'}...")

    finally:
        crawler.cleanup()


if __name__ == "__main__":
    try:
        test_step_by_step()
        test_full_process()
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()