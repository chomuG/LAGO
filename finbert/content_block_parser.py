# -*- coding: utf-8 -*-
"""
ìˆœì„œ ë³´ì¥ ì»¨í…ì¸  ë¸”ë¡ íŒŒì„œ
í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ì›ë³¸ ìˆœì„œëŒ€ë¡œ ë¸”ë¡ ë‹¨ìœ„ë¡œ íŒŒì‹±
"""

import requests
from requests.adapters import HTTPAdapter, Retry
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Optional, Tuple
import logging
from dataclasses import dataclass, asdict
import json
import re
import time

# Selenium imports for Google News redirect resolution
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.support.ui import WebDriverWait
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class ContentBlock:
    """ì»¨í…ì¸  ë¸”ë¡ ë°ì´í„° í´ë˜ìŠ¤"""
    type: str  # "text" or "image"
    content: str
    position: int
    caption: Optional[str] = None
    confidence: float = 1.0

@dataclass
class ParsedContent:
    """íŒŒì‹±ëœ ì»¨í…ì¸  ê²°ê³¼"""
    title: str
    blocks: List[ContentBlock]
    summary_text: str  # FinBERT ë¶„ì„ìš© í…ìŠ¤íŠ¸
    total_images: int
    parser_used: str
    confidence: float

class ContentBlockParser:
    """ìˆœì„œ ë³´ì¥ ì»¨í…ì¸  ë¸”ë¡ íŒŒì„œ"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                          'AppleWebKit/537.36 (KHTML, like Gecko) '
                          'Chrome/120.0.0.0 Safari/537.36'),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://news.google.com/',  # êµ¬ê¸€ ë‰´ìŠ¤ ê²½ìœ  í˜ì´ì§€ ëŒ€ì‘
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
        
        # HTTP ë¦¬íŠ¸ë¼ì´ ì„¤ì •
        retries = Retry(
            total=3,
            backoff_factor=0.5,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "HEAD", "OPTIONS"]
        )
        adapter = HTTPAdapter(max_retries=retries)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Selenium WebDriver for Google News redirect resolution
        self.driver = None
    
    def __del__(self):
        """ì†Œë©¸ì: Selenium WebDriver ì •ë¦¬"""
        self._cleanup_selenium()
    
    def _setup_selenium(self):
        """Selenium WebDriver ì„¤ì • (Google News ë¦¬ë””ë ‰íŠ¸ìš©)"""
        if not SELENIUM_AVAILABLE:
            logger.warning("Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Google News ë¦¬ë””ë ‰íŠ¸ í•´ê²°ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return False
            
        if self.driver is None:
            try:
                chrome_options = Options()
                chrome_options.add_argument("--headless")
                chrome_options.add_argument("--no-sandbox")
                chrome_options.add_argument("--disable-dev-shm-usage")
                chrome_options.add_argument("--disable-gpu")
                chrome_options.add_argument("--window-size=1920,1080")
                chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
                # ì•ˆì •ì„± ê°œì„  ì˜µì…˜
                chrome_options.add_argument("--disable-extensions")
                chrome_options.add_argument("--disable-plugins")
                chrome_options.add_argument("--disable-images")  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
                chrome_options.add_argument("--disable-javascript")  # JS ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ (ì¼ë¶€ ì‚¬ì´íŠ¸ì—ì„œëŠ” í•„ìš”)
                chrome_options.add_argument("--disable-web-security")
                chrome_options.add_argument("--ignore-certificate-errors")
                chrome_options.add_argument("--ignore-ssl-errors")
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                chrome_options.add_argument("--page-load-strategy=eager")  # ë¹ ë¥¸ ë¡œë”©

                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                logger.info("âœ… Selenium WebDriver ì„¤ì • ì™„ë£Œ (Google News ë¦¬ë””ë ‰íŠ¸ìš©)")
                return True
            except Exception as e:
                logger.error(f"Selenium ì„¤ì • ì˜¤ë¥˜: {e}")
                self.driver = None
                return False
        return True
    
    def _cleanup_selenium(self):
        """Selenium WebDriver ì •ë¦¬"""
        if self.driver:
            try:
                self.driver.quit()
                self.driver = None
                logger.debug("Selenium WebDriver ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"Selenium WebDriver ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def parse_url(self, url: str) -> ParsedContent:
        """URLì—ì„œ ìˆœì„œ ë³´ì¥ ì»¨í…ì¸  ë¸”ë¡ íŒŒì‹±"""
        logger.info(f"ì»¨í…ì¸  ë¸”ë¡ íŒŒì‹± ì‹œì‘: {url}")
        
        # Google News URLì¸ ê²½ìš° ì‹¤ì œ ê¸°ì‚¬ URLë¡œ ë¦¬ë””ë ‰ì…˜ í•´ê²°
        if 'news.google.com' in url:
            resolved_url = self._resolve_google_redirect(url)
            if resolved_url:
                logger.info(f"Google News ë¦¬ë””ë ‰ì…˜ í•´ê²°: {url[:50]}... -> {resolved_url[:50]}...")
                url = resolved_url
            else:
                logger.warning(f"Google News ë¦¬ë””ë ‰ì…˜ í•´ê²° ì‹¤íŒ¨, ì›ë³¸ URL ì‚¬ìš©: {url[:50]}...")
        
        try:
            # HTML ë‹¤ìš´ë¡œë“œ
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # ì¸ì½”ë”© êµì • - ì˜ëª» ê°ì§€ëœ ì¸ì½”ë”© ìˆ˜ì •
            if not response.encoding or response.encoding.lower() == 'iso-8859-1':
                # apparent_encoding ë˜ëŠ” utf-8 ì‚¬ìš©
                response.encoding = response.apparent_encoding or 'utf-8'
            
            # ì¼ë¶€ í•œêµ­ ì‚¬ì´íŠ¸ëŠ” euc-kr ì‚¬ìš©
            if 'euc-kr' in response.headers.get('Content-Type', '').lower():
                response.encoding = 'euc-kr'
            
            html = response.text
            
            # í¬í„¸ë³„ íŒŒì„œ ì„ íƒ (í™•ì¥ëœ ë„ë©”ì¸ ì»¤ë²„ë¦¬ì§€)
            domain = urlparse(url).netloc.lower()
            
            if 'naver.com' in domain or 'n.news.naver.com' in domain:
                result = self._parse_naver(html, url)
            elif 'hankyung.com' in domain:
                result = self._parse_hankyung(html, url)
            elif 'mk.co.kr' in domain:
                result = self._parse_maekyung(html, url)
            elif 'chosun.com' in domain or 'chosunbiz.com' in domain or 'biz.chosun.com' in domain:
                result = self._parse_chosun(html, url)
            elif 'daum.net' in domain or 'v.daum.net' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_daum êµ¬í˜„
            elif 'yna.co.kr' in domain or 'yonhapnews.co.kr' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_yonhap êµ¬í˜„
            elif 'donga.com' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_donga êµ¬í˜„
            elif 'joongang.co.kr' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_joongang êµ¬í˜„
            elif 'hani.co.kr' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_hani êµ¬í˜„
            elif 'khan.co.kr' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_khan êµ¬í˜„
            elif 'mt.co.kr' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_mt êµ¬í˜„
            elif 'edaily.co.kr' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_edaily êµ¬í˜„
            elif 'fnnews.com' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_fnnews êµ¬í˜„
            elif 'news1.kr' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_news1 êµ¬í˜„
            elif 'inews24.com' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_inews24 êµ¬í˜„
            elif 'etnews.com' in domain:
                result = self._parse_generic(html, url)  # í•„ìš”ì‹œ _parse_etnews êµ¬í˜„
            elif 'pinpointnews.co.kr' in domain:
                result = self._parse_pinpoint(html, url)  # pinpointnews ì „ìš© íŒŒì„œ
            else:
                result = self._parse_generic(html, url)
            
            logger.info(f"íŒŒì‹± ì™„ë£Œ: ì œëª©={result.title[:30]}..., ë¸”ë¡={len(result.blocks)}ê°œ, ì´ë¯¸ì§€={result.total_images}ê°œ")
            return result
            
        except Exception as e:
            logger.error(f"ì»¨í…ì¸  ë¸”ë¡ íŒŒì‹± ì‹¤íŒ¨ {url}: {e}")
            raise
    
    def _resolve_google_redirect(self, google_url: str) -> Optional[str]:
        """Google News ë¦¬ë””ë ‰ì…˜ URLì„ ì‹¤ì œ URLë¡œ í•´ê²° (Selenium ê¸°ë°˜)"""
        if 'news.google.com' not in google_url:
            return google_url
            
        try:
            # Seleniumìœ¼ë¡œ ë¦¬ë””ë ‰íŠ¸ ì²˜ë¦¬ (ì‚¬ìš©ìì˜ test.pyì—ì„œ ê²€ì¦ëœ ë°©ì‹)
            if not self.driver:
                if not self._setup_selenium():
                    logger.warning("Selenium ì‚¬ìš© ë¶ˆê°€, Google News ë¦¬ë””ë ‰íŠ¸ ê±´ë„ˆëœ€")
                    return None

            if not self.driver:
                return None

            logger.info(f"ğŸ”„ Seleniumìœ¼ë¡œ Google News ë¦¬ë””ë ‰íŠ¸ ì²˜ë¦¬: {google_url[:60]}...")
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            self.driver.set_page_load_timeout(15)
            self.driver.implicitly_wait(5)
            
            # Google News URLë¡œ ì´ë™
            self.driver.get(google_url)

            # ë¦¬ë””ë ‰íŠ¸ ëŒ€ê¸° (ìµœëŒ€ 15ì´ˆ) - Google Newsì—ì„œ ë²—ì–´ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                WebDriverWait(self.driver, 15).until(
                    lambda driver: 'news.google.com' not in driver.current_url and driver.current_url != 'about:blank'
                )
                # ì¶”ê°€ë¡œ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°
                time.sleep(1)  # í˜ì´ì§€ ì•ˆì •í™” ëŒ€ê¸°
            except Exception as e:
                logger.warning(f"Google News ë¦¬ë””ë ‰íŠ¸ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {e}")

            real_url = self.driver.current_url

            # ìµœì¢… URLì´ ìœ íš¨í•œ ë‰´ìŠ¤ í¬í„¸ì¸ì§€ í™•ì¸
            if 'news.google.com' not in real_url and self._is_valid_news_url(real_url):
                logger.info(f"âœ… Google News ë¦¬ë””ë ‰íŠ¸ ì„±ê³µ: {google_url[:50]}... -> {real_url[:60]}...")
                return real_url
            else:
                logger.warning(f"Google News ë¦¬ë””ë ‰íŠ¸ ì‹¤íŒ¨ (ì—¬ì „íˆ Google): {real_url[:60]}...")
                return None
                
        except Exception as e:
            logger.warning(f"Google News ë¦¬ë””ë ‰íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    def _is_valid_news_url(self, url: str) -> bool:
        """ìœ íš¨í•œ ë‰´ìŠ¤ URLì¸ì§€ í™•ì¸"""
        if not url or len(url) < 10:
            return False
        
        if not url.startswith(('http://', 'https://')):
            return False
        
        # Google ê´€ë ¨ URL ì œì™¸
        if any(domain in url.lower() for domain in ['google.com', 'googleusercontent.com', 'googlesyndication.com']):
            return False
        
        # í•œêµ­ ë‰´ìŠ¤ í¬í„¸ í™•ì¸
        korean_portals = [
            'naver.com', 'daum.net', 'hankyung.com', 'mk.co.kr', 'chosun.com', 
            'donga.com', 'joongang.co.kr', 'hani.co.kr', 'khan.co.kr', 'mt.co.kr',
            'edaily.co.kr', 'fnnews.com', 'news1.kr', 'inews24.com', 'etnews.com',
            'yonhapnews.co.kr', 'yna.co.kr', 'newsis.com', 'pinpointnews.co.kr'
        ]
        
        return any(portal in url.lower() for portal in korean_portals)
    
    def _parse_naver(self, html: str, url: str) -> ParsedContent:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© íŒŒì„œ - ìˆœì„œ ë³´ì¥"""
        soup = BeautifulSoup(html, 'lxml')
        
        # ì œëª© ì¶”ì¶œ
        title_elem = soup.select_one('h2.media_end_head_headline, h3.tts_head, .article_info h3')
        title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
        
        # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
        article_selectors = [
            'div#dic_area',
            'div#articeBody', 
            'div#newsEndContents',
            'div.news_end_main'
        ]
        
        article = None
        for selector in article_selectors:
            article = soup.select_one(selector)
            if article:
                break
        
        if not article:
            raise Exception("ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        # ë…¸ì´ì¦ˆ ì œê±°
        noise_selectors = [
            '.vod_area', '.link_news', '.reporter_area',
            '.copyright', '.promotion', '.news_function'
        ]
        for selector in noise_selectors:
            for elem in article.select(selector):
                elem.decompose()
        
        # ìˆœì„œ ë³´ì¥ ë¸”ë¡ íŒŒì‹±
        blocks = []
        position = 0
        text_parts = []
        
        # DOM ìˆœì„œëŒ€ë¡œ ìˆœíšŒí•˜ë©´ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì²˜ë¦¬
        for elem in article.find_all(['p', 'div', 'span', 'img'], recursive=False):
            
            if elem.name == 'img':
                # ì´ë¯¸ì§€ ë¸”ë¡ ì²˜ë¦¬
                img_block = self._process_naver_image(elem, position)
                if img_block:
                    blocks.append(img_block)
                    position += 1
            
            else:
                # í…ìŠ¤íŠ¸ ë¸”ë¡ ì²˜ë¦¬
                # í•˜ìœ„ ì´ë¯¸ì§€ ë¨¼ì € ì²˜ë¦¬
                for img in elem.find_all('img'):
                    img_block = self._process_naver_image(img, position)
                    if img_block:
                        blocks.append(img_block)
                        position += 1
                        # ì´ë¯¸ì§€ëŠ” í…ìŠ¤íŠ¸ì—ì„œ ì œê±°
                        img.decompose()
                
                # í…ìŠ¤íŠ¸ ì²˜ë¦¬
                text = elem.get_text(strip=True)
                if self._is_valid_text(text):
                    text_block = ContentBlock(
                        type="text",
                        content=text,
                        position=position,
                        confidence=0.95
                    )
                    blocks.append(text_block)
                    text_parts.append(text)
                    position += 1
        
        # ê²°ê³¼ êµ¬ì„±
        summary_text = '\n\n'.join(text_parts)
        total_images = len([b for b in blocks if b.type == "image"])
        
        return ParsedContent(
            title=title,
            blocks=blocks,
            summary_text=summary_text,
            total_images=total_images,
            parser_used="naver_specialized",
            confidence=0.95
        )
    
    def _parse_hankyung(self, html: str, url: str) -> ParsedContent:
        """í•œêµ­ê²½ì œ ì „ìš© íŒŒì„œ"""
        soup = BeautifulSoup(html, 'lxml')
        
        # ì œëª©
        title_elem = soup.select_one('h1.headline, .article-headline h1, .news-headline')
        title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
        
        # ë³¸ë¬¸
        article = soup.select_one('.article-body, .news-body, #articletxt')
        if not article:
            raise Exception("í•œêµ­ê²½ì œ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        return self._parse_generic_structure(article, title, "hankyung_specialized", 0.85)
    
    def _parse_maekyung(self, html: str, url: str) -> ParsedContent:
        """ë§¤ì¼ê²½ì œ ì „ìš© íŒŒì„œ"""
        soup = BeautifulSoup(html, 'lxml')
        
        # ì œëª©
        title_elem = soup.select_one('h1.news_ttl, .art_hd h1, .article_head h1')
        title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
        
        # ë³¸ë¬¸
        article = soup.select_one('.news_cnt_detail_wrap, .art_txt, .article_content')
        if not article:
            raise Exception("ë§¤ì¼ê²½ì œ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        return self._parse_generic_structure(article, title, "maekyung_specialized", 0.85)
    
    def _parse_chosun(self, html: str, url: str) -> ParsedContent:
        """ì¡°ì„ ì¼ë³´ ì „ìš© íŒŒì„œ"""
        soup = BeautifulSoup(html, 'lxml')
        
        # ì œëª©
        title_elem = soup.select_one('h1.article-title, .news_title_text, h1')
        title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
        
        # ë³¸ë¬¸
        article = soup.select_one('.article-body, .par, .news_body')
        if not article:
            raise Exception("ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        return self._parse_generic_structure(article, title, "chosun_specialized", 0.80)
    
    def _parse_pinpoint(self, html: str, url: str) -> ParsedContent:
        """í•€í¬ì¸íŠ¸ë‰´ìŠ¤ ì „ìš© íŒŒì„œ"""
        soup = BeautifulSoup(html, 'lxml')
        
        # ì œëª© ì¶”ì¶œ - ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        title = ""
        title_selectors = [
            'h1.article-title',
            'h1.news-title',
            'div.article-header h1',
            '.news-header h1',
            'h1',
            'title'
        ]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                if title and len(title) > 10:
                    break
        
        if not title or len(title) < 10:
            title = "ì œëª© ì—†ìŒ"
        
        # ë³¸ë¬¸ ì¶”ì¶œ - ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        content = ""
        article_selectors = [
            '.article-view',
            '.article-body',
            '.news-content',
            '.content-area',
            'article',
            'main',
            '#article-body',
            '.article_body'
        ]
        
        for selector in article_selectors:
            body = soup.select_one(selector)
            if body:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for tag in body.select('script, style, nav, aside, figure figcaption, .sns, .share, .ad, .advertisement, .related'):
                    tag.decompose()
                
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                paragraphs = []
                for elem in body.find_all(['p', 'div', 'li']):
                    text = elem.get_text(" ", strip=True)
                    if len(text) > 20 and not any(skip in text.lower() for skip in ['copyright', 'ë¬´ë‹¨ì „ì¬', 'ì¬ë°°í¬', 'ê¸°ì']):
                        paragraphs.append(text)
                
                temp_content = " ".join(paragraphs)
                if len(temp_content) > len(content):
                    content = temp_content
        
        # ë¸”ë¡ êµ¬ì¡° ìƒì„±
        blocks = []
        position = 0
        
        # í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ê°€
        if content:
            text_block = ContentBlock(
                type="text",
                content=content,
                position=position,
                confidence=0.85
            )
            blocks.append(text_block)
            position += 1
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        image_selectors = [
            'article img',
            '.article img',
            '.news_body img',
            '.article_body img',
            '.article-view img',
            'main img'
        ]
        
        for selector in image_selectors:
            for img in soup.select(selector):
                src = img.get('src') or img.get('data-src')
                if src and self._is_content_image(src):
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        from urllib.parse import urljoin
                        src = urljoin(url, src)
                    
                    img_block = ContentBlock(
                        type="image",
                        content=src,
                        position=position,
                        confidence=0.80
                    )
                    blocks.append(img_block)
                    position += 1
        
        # ê²°ê³¼ ìƒì„±
        summary_text = content[:500] if content else ""
        total_images = len([b for b in blocks if b.type == "image"])
        
        return ParsedContent(
            title=title,
            blocks=blocks,
            summary_text=summary_text,
            total_images=total_images,
            parser_used="pinpoint_specialized",
            confidence=0.85
        )
    
    def _parse_generic(self, html: str, url: str) -> ParsedContent:
        """ë²”ìš© íŒŒì„œ (ì§€ì›í•˜ì§€ ì•ŠëŠ” í¬í„¸)"""
        from readability import Document
        
        doc = Document(html)
        clean_html = doc.summary()
        soup = BeautifulSoup(clean_html, 'lxml')
        
        title = doc.title() or "ì œëª© ì—†ìŒ"
        
        return self._parse_generic_structure(soup, title, "generic_parser", 0.60)
    
    def _parse_generic_structure(self, article_elem, title: str, parser_name: str, confidence: float) -> ParsedContent:
        """ë²”ìš© êµ¬ì¡° íŒŒì„œ"""
        blocks = []
        position = 0
        text_parts = []
        
        # ëª¨ë“  p, div, img íƒœê·¸ë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        for elem in article_elem.find_all(['p', 'div', 'img', 'figure']):
            
            if elem.name == 'img':
                img_block = self._process_generic_image(elem, position)
                if img_block:
                    blocks.append(img_block)
                    position += 1
            
            elif elem.name == 'figure':
                # figure ë‚´ë¶€ ì´ë¯¸ì§€ ì²˜ë¦¬
                img = elem.find('img')
                if img:
                    caption_elem = elem.find('figcaption')
                    caption = caption_elem.get_text(strip=True) if caption_elem else None
                    
                    img_block = self._process_generic_image(img, position, caption)
                    if img_block:
                        blocks.append(img_block)
                        position += 1
            
            else:
                # í…ìŠ¤íŠ¸ ì²˜ë¦¬ (p, div)
                text = elem.get_text(strip=True)
                if self._is_valid_text(text):
                    text_block = ContentBlock(
                        type="text",
                        content=text,
                        position=position,
                        confidence=confidence
                    )
                    blocks.append(text_block)
                    text_parts.append(text)
                    position += 1
        
        summary_text = '\n\n'.join(text_parts)
        total_images = len([b for b in blocks if b.type == "image"])
        
        return ParsedContent(
            title=title,
            blocks=blocks,
            summary_text=summary_text,
            total_images=total_images,
            parser_used=parser_name,
            confidence=confidence
        )
    
    def _process_naver_image(self, img_elem, position: int) -> Optional[ContentBlock]:
        """ë„¤ì´ë²„ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        src = img_elem.get('data-src') or img_elem.get('src')
        if not src or not self._is_content_image(src):
            return None
        
        # ë„¤ì´ë²„ ì´ë¯¸ì§€ URL ì •ê·œí™”
        if src.startswith('//'):
            src = 'https:' + src
        elif src.startswith('/'):
            src = 'https://imgnews.pstatic.net' + src
        
        # ìº¡ì…˜ ì°¾ê¸°
        caption = self._find_naver_caption(img_elem)
        
        return ContentBlock(
            type="image",
            content=src,
            position=position,
            caption=caption,
            confidence=0.90
        )
    
    def _process_generic_image(self, img_elem, position: int, caption: str = None) -> Optional[ContentBlock]:
        """ë²”ìš© ì´ë¯¸ì§€ ì²˜ë¦¬"""
        src = img_elem.get('src') or img_elem.get('data-src')
        if not src or not self._is_content_image(src):
            return None
        
        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
        if src.startswith('//'):
            src = 'https:' + src
        elif src.startswith('/'):
            # ë² ì´ìŠ¤ ë„ë©”ì¸ ì¶”ê°€ í•„ìš”í•œ ê²½ìš°
            pass
        
        return ContentBlock(
            type="image",
            content=src,
            position=position,
            caption=caption,
            confidence=0.70
        )
    
    def _is_content_image(self, src: str) -> bool:
        """ë³¸ë¬¸ ì´ë¯¸ì§€ì¸ì§€ íŒë³„"""
        if not src:
            return False
        
        # ì œì™¸í•  ì´ë¯¸ì§€ íŒ¨í„´
        exclude_patterns = [
            'icon', 'logo', 'banner', 'ad', 'btn',
            'arrow', 'bullet', 'emoticon', 'profile'
        ]
        
        src_lower = src.lower()
        return not any(pattern in src_lower for pattern in exclude_patterns)
    
    def _is_valid_text(self, text: str) -> bool:
        """ìœ íš¨í•œ í…ìŠ¤íŠ¸ì¸ì§€ íŒë³„"""
        if not text or len(text) < 20:
            return False
        
        # ì œì™¸í•  í…ìŠ¤íŠ¸ íŒ¨í„´
        exclude_patterns = [
            r'^[\s\n]*$',  # ê³µë°±ë§Œ
            r'â–¶.*ë°”ë¡œê°€ê¸°',
            r'Copyright.*reserved',
            r'^.*@.*\.(com|co\.kr)$',  # ì´ë©”ì¼ë§Œ
            r'^\d{4}-\d{2}-\d{2}.*\d{2}:\d{2}$'  # ë‚ ì§œì‹œê°„ë§Œ
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return False
        
        return True
    
    def _find_naver_caption(self, img_elem) -> Optional[str]:
        """ë„¤ì´ë²„ ì´ë¯¸ì§€ ìº¡ì…˜ ì°¾ê¸°"""
        # ë‹¤ìŒ í˜•ì œ ìš”ì†Œì—ì„œ ìº¡ì…˜ ì°¾ê¸°
        next_elem = img_elem.find_next_sibling(['em', 'span', 'p'])
        if next_elem:
            caption = next_elem.get_text(strip=True)
            if len(caption) < 100 and any(word in caption for word in ['ì‚¬ì§„', 'ì œê³µ', 'ì¶œì²˜']):
                return caption
        
        # ë¶€ëª¨ ìš”ì†Œì˜ ìº¡ì…˜ ì°¾ê¸°
        parent = img_elem.parent
        if parent:
            caption_elem = parent.find('figcaption') or parent.find('.caption')
            if caption_elem:
                return caption_elem.get_text(strip=True)
        
        return None
    
    def to_json(self, parsed_content: ParsedContent) -> str:
        """ParsedContentë¥¼ JSONìœ¼ë¡œ ë³€í™˜"""
        data = {
            'title': parsed_content.title,
            'blocks': [asdict(block) for block in parsed_content.blocks],
            'summary_text': parsed_content.summary_text,
            'total_images': parsed_content.total_images,
            'parser_used': parsed_content.parser_used,
            'confidence': parsed_content.confidence
        }
        return json.dumps(data, ensure_ascii=False, indent=2)


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    parser = ContentBlockParser()
    
    # ë„¤ì´ë²„ ë‰´ìŠ¤ í…ŒìŠ¤íŠ¸
    test_url = "https://n.news.naver.com/mnews/article/022/0003872341"
    
    try:
        result = parser.parse_url(test_url)
        
        print(f"ì œëª©: {result.title}")
        print(f"ì´ ë¸”ë¡: {len(result.blocks)}ê°œ")
        print(f"ì´ë¯¸ì§€: {result.total_images}ê°œ")
        print(f"íŒŒì„œ: {result.parser_used}")
        print(f"ì‹ ë¢°ë„: {result.confidence}")
        print("\n=== ë¸”ë¡ ìˆœì„œ ===")
        
        for i, block in enumerate(result.blocks[:5]):  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
            if block.type == "text":
                print(f"{i+1}. [TEXT] {block.content[:50]}...")
            else:
                print(f"{i+1}. [IMAGE] {block.content}")
                if block.caption:
                    print(f"    ìº¡ì…˜: {block.caption}")
        
        print(f"\n=== ìš”ì•½ í…ìŠ¤íŠ¸ ===")
        print(result.summary_text[:200] + "...")
        
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")