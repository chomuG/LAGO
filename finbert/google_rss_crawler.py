# google_rss_crawler.py
import feedparser
import requests
from datetime import datetime, timedelta
from typing import List, Optional
from dataclasses import dataclass
import logging
import re
from urllib.parse import urlparse, parse_qs, unquote, urlencode
import urllib.parse
import threading

# Selenium ê´€ë ¨ importë“¤ (configì—ì„œ RENDER_JS=Trueì¼ ë•Œë§Œ ì‚¬ìš©)
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service as ChromeService
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    logger.warning("Selenium not available. JavaScript rendering disabled.")

logger = logging.getLogger(__name__)


@dataclass
class NewsItem:
    """ë‰´ìŠ¤ ì•„ì´í…œ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    url: str
    source: str
    published_at: datetime
    category: str = "ê²½ì œ"
    symbol: Optional[str] = None


class GoogleRSSCrawler:
    """Google RSS ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.base_url = "https://news.google.com/rss/search"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # ìŠ¤ë ˆë“œë³„ WebDriver ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
        self._local = threading.local()
        self.webdriver_timeout = 3  # Google News í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒì„ 3ì´ˆë¡œ ë‹¨ì¶•

    def get_webdriver(self):
        """ìŠ¤ë ˆë“œë³„ WebDriver ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        from config import RENDER_JS, JS_RENDER_TIMEOUT
        
        if not RENDER_JS or not SELENIUM_AVAILABLE:
            return None
            
        if not hasattr(self._local, 'driver'):
            try:
                chrome_options = ChromeOptions()
                chrome_options.add_argument('--headless')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                chrome_options.add_argument('--disable-gpu')
                chrome_options.add_argument('--window-size=1920,1080')
                chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36')
                
                # Docker í™˜ê²½ ëŒ€ì‘
                chrome_options.add_argument('--remote-debugging-port=9222')
                chrome_options.add_argument('--disable-extensions')
                
                service = ChromeService(ChromeDriverManager().install())
                self._local.driver = webdriver.Chrome(service=service, options=chrome_options)
                self._local.driver.set_page_load_timeout(JS_RENDER_TIMEOUT)
                
                logger.debug(f"ìƒˆ WebDriver ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - ìŠ¤ë ˆë“œ: {threading.current_thread().name}")
                
            except Exception as e:
                logger.error(f"WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self._local.driver = None
                
        return getattr(self._local, 'driver', None)
    
    def extract_real_url_with_selenium(self, google_url: str) -> Optional[str]:
        """Seleniumì„ ì‚¬ìš©í•œ JavaScript ê¸°ë°˜ ë¦¬ë””ë ‰ì…˜ ì²˜ë¦¬"""
        logger.debug(f"ğŸ” Selenium í•¨ìˆ˜ í˜¸ì¶œë¨: {google_url[:80]}...")
        
        driver = self.get_webdriver()
        if not driver:
            logger.warning("âš ï¸ WebDriverë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ, ì¼ë°˜ HTTP ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´")
            return None
            
        logger.debug("âœ… WebDriver íšë“ ì„±ê³µ")
            
        try:
            logger.debug(f"Seleniumìœ¼ë¡œ JavaScript ë¦¬ë””ë ‰ì…˜ ì²˜ë¦¬: {google_url[:80]}...")
            
            # Google News í˜ì´ì§€ ë¡œë“œ
            logger.debug(f"í˜ì´ì§€ ë¡œë“œ ì‹œì‘: {google_url[:80]}...")
            driver.get(google_url)
            
            # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            import time
            time.sleep(3)  # 3ì´ˆ ëŒ€ê¸°
            
            # 1ì°¨: JavaScript ì‹¤í–‰ í›„ í˜„ì¬ URL í™•ì¸
            final_url = driver.current_url
            logger.debug(f"í˜„ì¬ URL: {final_url[:80]}...")
            
            # Google ë„ë©”ì¸ì´ ì•„ë‹ˆë©´ ì„±ê³µ
            if not any(domain in final_url for domain in ['google.com', 'googleadservices.com']):
                logger.info(f"âœ… Selenium URL ë³€ê²½ ê°ì§€: {final_url[:80]}...")
                return final_url
            
            # 2ì°¨: í˜ì´ì§€ ë‚´ ì‹¤ì œ ë‰´ìŠ¤ ë§í¬ ì ê·¹ì ìœ¼ë¡œ ì°¾ê¸°
            try:
                # ë‹¤ì–‘í•œ ì„ íƒìë¡œ ë§í¬ ì°¾ê¸°
                selectors = [
                    'a[href*="://"]',  # ëª¨ë“  ì™¸ë¶€ ë§í¬
                    'article a',       # ê¸°ì‚¬ ë‚´ ë§í¬
                    '.article a',      # ê¸°ì‚¬ í´ë˜ìŠ¤ ë‚´ ë§í¬
                    '[data-n-tid] a',  # Google News íŠ¹ì • ì†ì„±
                    '.VDXfz a',        # Google News CSS í´ë˜ìŠ¤
                    '.WwrzSb a',       # Google News CSS í´ë˜ìŠ¤
                    '.ipQwMb a'        # Google News CSS í´ë˜ìŠ¤
                ]
                
                for selector in selectors:
                    try:
                        links = driver.find_elements(By.CSS_SELECTOR, selector)
                        logger.debug(f"ì„ íƒì '{selector}'ë¡œ {len(links)}ê°œ ë§í¬ ë°œê²¬")
                        
                        for link in links[:5]:  # ìƒìœ„ 5ê°œë§Œ í™•ì¸
                            try:
                                href = link.get_attribute('href')
                                if href and self._is_news_url(href):
                                    logger.info(f"âœ… ì‹¤ì œ ë‰´ìŠ¤ ë§í¬ ë°œê²¬: {href[:80]}...")
                                    return href
                            except:
                                continue
                                
                    except Exception as e:
                        logger.debug(f"ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")
                        continue
                        
            except Exception as e:
                logger.debug(f"ë§í¬ ê²€ìƒ‰ ì „ì²´ ì‹¤íŒ¨: {e}")
                
            logger.debug(f"ì‹¤ì œ ë‰´ìŠ¤ ë§í¬ ì°¾ê¸° ì‹¤íŒ¨: {final_url[:80]}...")
            return None
                
        except TimeoutException:
            logger.warning(f"Selenium íƒ€ì„ì•„ì›ƒ: {google_url[:80]}...")
            return None
        except WebDriverException as e:
            logger.warning(f"WebDriver ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            logger.error(f"Selenium ë¦¬ë””ë ‰ì…˜ ì˜¤ë¥˜: {e}")
            return None

    def _is_news_url(self, url: str) -> bool:
        """ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ URLì¸ì§€ í™•ì¸"""
        if not url or not isinstance(url, str):
            return False
            
        # Google ë„ë©”ì¸ ì œì™¸
        google_domains = ['google.com', 'googleadservices.com', 'googlesyndication.com', 'gstatic.com']
        if any(domain in url for domain in google_domains):
            return False
            
        # ì‹¤ì œ ë‰´ìŠ¤ ë„ë©”ì¸ í™•ì¸
        news_indicators = ['.co.kr', '.com', '.net', '.org', 'news', 'media', 'press']
        return any(indicator in url for indicator in news_indicators) and url.startswith('http')

    def extract_real_url_from_google(self, google_url: str) -> str:
        """Google News URLì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ - Smart ë°©ë²•"""
        try:
            # Google News URLì´ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if 'news.google.com' not in google_url:
                return google_url

            # ë°©ë²• 1: URL íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ (ë¹ ë¥¸ ë°©ë²•)
            parsed = urlparse(google_url)
            params = parse_qs(parsed.query)

            # 'url' íŒŒë¼ë¯¸í„° í™•ì¸
            if 'url' in params:
                extracted_url = unquote(params['url'][0])
                if not 'google.com' in extracted_url:
                    logger.debug(f"íŒŒë¼ë¯¸í„°ì—ì„œ URL ì¶”ì¶œ ì„±ê³µ: {extracted_url[:80]}...")
                    return extracted_url
            
            # Google News articles URL íŒ¨í„´ ì²˜ë¦¬
            if '/articles/' in google_url:
                # CBMië¡œ ì‹œì‘í•˜ëŠ” base64 ì¸ì½”ë”©ëœ URL ì°¾ê¸°
                import re
                match = re.search(r'/articles/(CBMi[^?&/]+)', google_url)
                if match:
                    try:
                        import base64
                        encoded_part = match.group(1)
                        # CBMi ì ‘ë‘ì‚¬ ì œê±°í•˜ê³  ë””ì½”ë”©
                        if encoded_part.startswith('CBMi'):
                            encoded_part = encoded_part[4:]
                        # base64 íŒ¨ë”© ì¶”ê°€
                        padding = 4 - (len(encoded_part) % 4)
                        if padding != 4:
                            encoded_part += '=' * padding
                        decoded = base64.b64decode(encoded_part).decode('utf-8')
                        if decoded.startswith('http') and not 'google.com' in decoded:
                            logger.info(f"âœ… base64 URL ë””ì½”ë”© ì„±ê³µ: {decoded[:80]}...")
                            return decoded
                    except Exception as e:
                        logger.debug(f"base64 ë””ì½”ë”© ì‹¤íŒ¨: {e}")

            # ë°©ë²• 2: GET ìš”ì²­ìœ¼ë¡œ ì‹¤ì œ ë¦¬ë””ë ‰ì…˜ ì¶”ì  (HEADë³´ë‹¤ íš¨ê³¼ì )
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
                
                # GET ìš”ì²­ìœ¼ë¡œ ëª¨ë“  ë¦¬ë””ë ‰ì…˜ ìë™ ì¶”ì 
                response = requests.get(google_url, allow_redirects=True, timeout=10, headers=headers)
                final_url = response.url
                
                # Google ë„ë©”ì¸ì´ ì•„ë‹ˆë©´ ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ URL
                if not any(domain in final_url for domain in ['google.com', 'googleadservices.com', 'googlesyndication.com']):
                    logger.info(f"âœ… HTTP GET ë¦¬ë””ë ‰ì…˜ ì„±ê³µ: {final_url[:80]}...")
                    return final_url
                else:
                    logger.debug(f"ì—¬ì „íˆ Google ë„ë©”ì¸: {final_url[:80]}...")
            except Exception as e:
                logger.debug(f"GET ë¦¬ë””ë ‰ì…˜ ì‹¤íŒ¨: {e}")

            # ë°©ë²• 3: Seleniumì„ ì‚¬ìš©í•œ JavaScript ê¸°ë°˜ ë¦¬ë””ë ‰ì…˜ (ìµœí›„ì˜ ìˆ˜ë‹¨)
            logger.debug(f"Selenium ë¦¬ë””ë ‰ì…˜ ì‹œë„ ì‹œì‘: {google_url[:80]}...")
            selenium_url = self.extract_real_url_with_selenium(google_url)
            if selenium_url:
                logger.info(f"âœ… Selenium ë¦¬ë””ë ‰ì…˜ ì„±ê³µ: {selenium_url[:80]}...")
                return selenium_url
            else:
                logger.debug("Selenium ë¦¬ë””ë ‰ì…˜ ì‹¤íŒ¨, HTTP ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´")
                
            # ë°©ë²• 3: GET ìš”ì²­ìœ¼ë¡œ ì™„ì „í•œ ë¦¬ë””ë ‰ì…˜ ì¶”ì  (Fallback)
            try:
                # ì‹¤ì œ ë¸Œë¼ìš°ì € í—¤ë”ë¡œ GET ìš”ì²­
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
                
                # GET ìš”ì²­ìœ¼ë¡œ ëª¨ë“  ë¦¬ë””ë ‰ì…˜ ìë™ ì¶”ì 
                response = requests.get(google_url, allow_redirects=True, timeout=15, headers=headers)
                final_url = response.url
                
                logger.debug(f"GET ë¦¬ë””ë ‰ì…˜ ê²°ê³¼: {google_url[:80]} -> {final_url[:80]}...")

                # Google ë„ë©”ì¸ì´ ì•„ë‹ˆë©´ ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ URL
                if not any(domain in final_url for domain in ['google.com', 'googleadservices.com', 'googlesyndication.com']):
                    logger.info(f"ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ ì„±ê³µ: {final_url[:80]}...")
                    return final_url
                else:
                    logger.debug(f"ì—¬ì „íˆ Google ë„ë©”ì¸: {final_url[:80]}...")

            except Exception as e:
                logger.debug(f"GET ë¦¬ë””ë ‰ì…˜ ì‹¤íŒ¨: {e}")

            # ë°©ë²• 4: base64 ë””ì½”ë”© ì‹œë„ (ëŒ€ì•ˆ)
            if '/articles/' in google_url:
                try:
                    import base64
                    # URLì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ì¶œ
                    parts = google_url.split('/')
                    for part in parts:
                        if len(part) > 20:  # base64 ì¸ì½”ë”©ëœ ë¶€ë¶„ì€ ë³´í†µ ê¸¸ë‹¤
                            try:
                                decoded = base64.b64decode(part + '==').decode('utf-8')
                                if decoded.startswith('http') and not 'google.com' in decoded:
                                    logger.debug(f"base64 ë””ì½”ë”© ì„±ê³µ: {decoded[:80]}...")
                                    return decoded
                            except:
                                continue
                except:
                    pass

            # ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë°˜í™˜
            logger.warning(f"ì‹¤ì œ URL ì¶”ì¶œ ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {google_url[:80]}...")
            return google_url

        except Exception as e:
            logger.error(f"URL ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return google_url

    def parse_rss_feed(self, rss_url: str, limit: int = 10) -> List[NewsItem]:
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            logger.info(f"RSS í”¼ë“œ íŒŒì‹±: {rss_url[:100]}...")
            logger.debug(f"ì „ì²´ RSS URL: {rss_url}")

            # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
            feed = feedparser.parse(rss_url)
            
            # í”¼ë“œ ìƒíƒœ ë””ë²„ê¹…
            logger.debug(f"í”¼ë“œ ìƒíƒœ - status: {getattr(feed, 'status', 'N/A')}")
            logger.debug(f"í”¼ë“œ ë²„ì „: {getattr(feed, 'version', 'N/A')}")
            logger.debug(f"í”¼ë“œ ì—ëŸ¬: {getattr(feed, 'bozo', False)}")
            if hasattr(feed, 'bozo_exception'):
                logger.error(f"í”¼ë“œ íŒŒì‹± ì—ëŸ¬: {feed.bozo_exception}")

            if not feed.entries:
                logger.warning(f"RSS í”¼ë“œì— í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤ - entries ìˆ˜: {len(feed.entries)}")
                # í”¼ë“œ ë‚´ìš© ì¼ë¶€ ì¶œë ¥
                if hasattr(feed, 'feed'):
                    logger.debug(f"í”¼ë“œ ì œëª©: {getattr(feed.feed, 'title', 'N/A')}")
                    logger.debug(f"í”¼ë“œ ë§í¬: {getattr(feed.feed, 'link', 'N/A')}")
                return []

            news_items = []
            processed_urls = set()  # ì¤‘ë³µ URL ì²´í¬

            for entry in feed.entries[:limit * 2]:  # ì—¬ìœ ìˆê²Œ ê°€ì ¸ì˜¤ê¸°
                try:
                    # Google News URL
                    google_url = entry.get('link', '')

                    if not google_url:
                        continue

                    # ì‹¤ì œ URL ì¶”ì¶œ
                    real_url = self.extract_real_url_from_google(google_url)

                    # Google News í˜ì´ì§€ ì²˜ë¦¬ ì™„í™” - ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ URLë§Œ ìš°ì„  ì²˜ë¦¬
                    if 'news.google.com' in real_url:
                        # ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ URLì„ ëª» ì°¾ì€ ê²½ìš°ì—ë§Œ Google News URLë„ í—ˆìš©
                        logger.debug(f"Google News URL ë°œê²¬, ê³„ì† ì²˜ë¦¬: {real_url[:80]}...")
                        # continue ì œê±° - Google News URLë„ ì²˜ë¦¬í•˜ë„ë¡ ë³€ê²½

                    # ì¤‘ë³µ ì²´í¬
                    if real_url in processed_urls:
                        continue
                    processed_urls.add(real_url)

                    # ì œëª© ì¶”ì¶œ
                    title = entry.get('title', '').strip()
                    if not title or len(title) < 10:
                        continue

                    # ì†ŒìŠ¤ ì¶”ì¶œ (Google RSSëŠ” ì œëª©ì— ì†ŒìŠ¤ í¬í•¨)
                    source = "Unknown"
                    if ' - ' in title:
                        parts = title.rsplit(' - ', 1)
                        if len(parts) == 2:
                            title = parts[0]
                            source = parts[1]

                    # ë°œí–‰ ì‹œê°„
                    published = entry.get('published_parsed', None)
                    if published:
                        published_at = datetime.fromtimestamp(
                            datetime(*published[:6]).timestamp()
                        )
                    else:
                        published_at = datetime.now()

                    news_item = NewsItem(
                        title=title,
                        url=real_url,
                        source=source,
                        published_at=published_at,
                        category="ê²½ì œ"
                    )

                    news_items.append(news_item)
                    logger.debug(f"ë‰´ìŠ¤ ì¶”ê°€: {title[:50]}... ({source})")

                    if len(news_items) >= limit:
                        break

                except Exception as e:
                    logger.error(f"í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            logger.info(f"RSS í”¼ë“œì—ì„œ {len(news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            return news_items

        except Exception as e:
            logger.error(f"RSS í”¼ë“œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

    def get_realtime_news(self, limit: int = 20) -> List[NewsItem]:
        """ì‹¤ì‹œê°„ ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            # í•œêµ­ ê²½ì œ ë‰´ìŠ¤ ê²€ìƒ‰ì–´ë“¤
            queries = [
                "ê²½ì œ OR ì¦ì‹œ OR ì£¼ì‹",
                "ì½”ìŠ¤í”¼ OR ì½”ìŠ¤ë‹¥",
                "ê¸ˆìœµ OR íˆ¬ì"
            ]

            all_news = []
            items_per_query = max(limit // len(queries), 5)

            for query in queries:
                params = {
                    'q': query,
                    'hl': 'ko',
                    'gl': 'KR',
                    'ceid': 'KR:ko'
                }

                # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ì•ˆì „í•˜ê²Œ URLë¡œ ë³€í™˜ (URL ì¸ì½”ë”© í¬í•¨)
                query_string = urllib.parse.urlencode(params)
                rss_url = f"{self.base_url}?{query_string}"

                news_items = self.parse_rss_feed(rss_url, items_per_query)
                all_news.extend(news_items)

            # ì¤‘ë³µ ì œê±° ë° ì‹œê°„ìˆœ ì •ë ¬
            seen_urls = set()
            unique_news = []

            for item in sorted(all_news, key=lambda x: x.published_at, reverse=True):
                if item.url not in seen_urls:
                    seen_urls.add(item.url)
                    unique_news.append(item)

            return unique_news[:limit]

        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def get_watchlist_news(self, symbol: str, company_name: str,
                           aliases: List[str], limit: int = 5) -> List[NewsItem]:
        """ê´€ì‹¬ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            # ê²€ìƒ‰ì–´ êµ¬ì„±
            search_terms = [company_name] + (aliases if aliases else [])
            query = ' OR '.join(f'"{term}"' for term in search_terms)

            params = {
                'q': query,
                'hl': 'ko',
                'gl': 'KR',
                'ceid': 'KR:ko'
            }

            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ì•ˆì „í•˜ê²Œ URLë¡œ ë³€í™˜ (URL ì¸ì½”ë”© í¬í•¨)
            query_string = urllib.parse.urlencode(params)
            rss_url = f"{self.base_url}?{query_string}"

            news_items = self.parse_rss_feed(rss_url, limit)

            # ì¢…ëª© ì •ë³´ ì„¤ì •
            for item in news_items:
                item.symbol = symbol
                item.category = "ê´€ì‹¬ì¢…ëª©"

            return news_items

        except Exception as e:
            logger.error(f"ê´€ì‹¬ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ {company_name}: {e}")
            return []

    def get_historical_news(self, symbol: str, company_name: str,
                            date: str, aliases: List[str], limit: int = 10) -> List[NewsItem]:
        """íŠ¹ì • ë‚ ì§œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (í•´ë‹¹ì¼ Â± 1ì¼)
            target_date = datetime.strptime(date, "%Y-%m-%d")
            date_before = (target_date - timedelta(days=1)).strftime("%Y-%m-%d")
            date_after = (target_date + timedelta(days=1)).strftime("%Y-%m-%d")

            # ê²€ìƒ‰ì–´ êµ¬ì„±
            search_terms = [company_name] + (aliases if aliases else [])
            query = ' OR '.join(f'"{term}"' for term in search_terms)
            query += f' after:{date_before} before:{date_after}'

            params = {
                'q': query,
                'hl': 'ko',
                'gl': 'KR',
                'ceid': 'KR:ko'
            }

            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ì•ˆì „í•˜ê²Œ URLë¡œ ë³€í™˜ (URL ì¸ì½”ë”© í¬í•¨)
            query_string = urllib.parse.urlencode(params)
            rss_url = f"{self.base_url}?{query_string}"

            news_items = self.parse_rss_feed(rss_url, limit)

            # ì¢…ëª© ì •ë³´ ì„¤ì •
            for item in news_items:
                item.symbol = symbol
                item.category = "ì—­ì‚¬ì±Œë¦°ì§€"

            return news_items

        except Exception as e:
            logger.error(f"ì—­ì‚¬ì  ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ {company_name} ({date}): {e}")
            return []

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.session.close()
        except:
            pass
            
        # WebDriver ì •ë¦¬
        if hasattr(self._local, 'driver') and self._local.driver:
            try:
                self._local.driver.quit()
                logger.debug("WebDriver ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"WebDriver ì •ë¦¬ ì˜¤ë¥˜: {e}")
            finally:
                del self._local.driver