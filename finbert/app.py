from flask import Flask, request, jsonify
import logging
from typing import List, Optional, Dict
import threading
import time
from functools import wraps
import requests
from datetime import datetime, timedelta

# ê¸°ì¡´ importë“¤
from config import HOST, PORT, DEBUG, LOG_LEVEL, LOG_FORMAT, TEXT_MAX_LENGTH
from news_crawler import extract_news_content_with_title
from sentiment_analysis import load_model, analyze_sentiment, is_model_loaded
from google_rss_crawler import GoogleRSSCrawler, NewsItem
from content_block_parser import ContentBlockParser
from parallel_processor import ParallelNewsProcessor  # ë³‘ë ¬ ì²˜ë¦¬ ì¶”ê°€
try:
    from cache_manager import cache_manager  # Redis ê¸°ë°˜ ìºì‹œ ë§¤ë‹ˆì €
except ImportError:
    from simple_cache import cache_manager  # Redis ì—†ì„ ë•Œ ë©”ëª¨ë¦¬ ìºì‹œ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=LOG_LEVEL, format=LOG_FORMAT)
logger = logging.getLogger(__name__)

app = Flask(__name__)


class StrictSequentialNewsProcessor:
    """ì—„ê²©í•œ ìˆœì°¨ ì²˜ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)"""

    def __init__(self):
        self._local = threading.local()
        self.claude_request_delay = 3.0  # Claude API ìš”ì²­ ê°„ 3ì´ˆ ëŒ€ê¸°
        self.last_claude_request = 0
        self.url_redirect_timeout = 10  # URL ë¦¬ë””ë ‰ì…˜ íƒ€ì„ì•„ì›ƒ
        self.min_content_length = 200  # ìµœì†Œ ì½˜í…ì¸  ê¸¸ì´
        self.collection_stats = {}  # ìˆ˜ì§‘ í†µê³„
        self.parallel_processor = ParallelNewsProcessor(max_workers=4)  # ë³‘ë ¬ ì²˜ë¦¬ ì—”ì§„
        self.enable_parallel = True  # ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” í”Œë˜ê·¸

    def get_crawler(self):
        """ìŠ¤ë ˆë“œë³„ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if not hasattr(self._local, 'crawler'):
            try:
                self._local.crawler = GoogleRSSCrawler()
                logger.debug(f"ìƒˆ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - ìŠ¤ë ˆë“œ: {threading.current_thread().name}")
            except Exception as e:
                logger.error(f"í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                raise
        return self._local.crawler

    def get_content_parser(self):
        """ìŠ¤ë ˆë“œë³„ ì½˜í…ì¸  íŒŒì„œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if not hasattr(self._local, 'content_parser'):
            try:
                self._local.content_parser = ContentBlockParser()
                logger.debug(f"ìƒˆ ì½˜í…ì¸  íŒŒì„œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - ìŠ¤ë ˆë“œ: {threading.current_thread().name}")
            except Exception as e:
                logger.error(f"ì½˜í…ì¸  íŒŒì„œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
                raise
        return self._local.content_parser

    def cleanup_local_resources(self):
        """í˜„ì¬ ìŠ¤ë ˆë“œì˜ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self._local, 'crawler'):
            try:
                if hasattr(self._local.crawler, 'cleanup'):
                    self._local.crawler.cleanup()
                logger.debug(f"í¬ë¡¤ëŸ¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"í¬ë¡¤ëŸ¬ ì •ë¦¬ ì˜¤ë¥˜: {e}")
            finally:
                del self._local.crawler

        if hasattr(self._local, 'content_parser'):
            try:
                if hasattr(self._local.content_parser, 'cleanup'):
                    self._local.content_parser.cleanup()
                del self._local.content_parser
            except Exception as e:
                logger.error(f"ì½˜í…ì¸  íŒŒì„œ ì •ë¦¬ ì˜¤ë¥˜: {e}")

    def wait_for_claude_rate_limit(self):
        """Claude API Rate Limiting"""
        current_time = time.time()
        elapsed = current_time - self.last_claude_request

        if elapsed < self.claude_request_delay:
            wait_time = self.claude_request_delay - elapsed
            logger.info(f"â³ Claude API Rate Limit: {wait_time:.1f}ì´ˆ ëŒ€ê¸°")
            time.sleep(wait_time)

        self.last_claude_request = time.time()

    def verify_and_redirect_url(self, url: str, index: int, total: int) -> Optional[str]:
        """URL ìœ íš¨ì„± ê²€ì¦ ë° ë¦¬ë””ë ‰ì…˜ ì²˜ë¦¬ (ìºì‹± ì ìš©)"""
        try:
            # ìºì‹œ í™•ì¸
            cached_url = cache_manager.get_url_redirect_cache(url)
            if cached_url:
                logger.info(f"ğŸ¯ [{index}/{total}] URL ìºì‹œ íˆíŠ¸")
                return cached_url if cached_url != "FAILED" else None
            
            logger.info(f"ğŸ”— [{index}/{total}] URL ê²€ì¦ ë° ë¦¬ë””ë ‰ì…˜ ì¤‘...")
            logger.debug(f"   ì›ë³¸ RSS URL: {url}")

            response = requests.head(url, allow_redirects=True, timeout=self.url_redirect_timeout)
            final_url = response.url

            if response.status_code >= 400:
                logger.error(f"âŒ [{index}/{total}] URL ì ‘ê·¼ ì‹¤íŒ¨ (HTTP {response.status_code})")
                # ì‹¤íŒ¨ë„ ìºì‹± (ì§§ì€ ì‹œê°„)
                cache_manager.set_url_redirect_cache(url, "FAILED", ttl=300)  # 5ë¶„
                return None

            # URLì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if final_url != url:
                logger.info(f"âœ… [{index}/{total}] URL ë¦¬ë””ë ‰ì…˜ ì™„ë£Œ")
                logger.info(f"   RSS URL: {url[:80]}...")
                logger.info(f"   ì‹¤ì œ URL: {final_url[:80]}...")
            else:
                logger.info(f"âœ… [{index}/{total}] URL ê²€ì¦ ì™„ë£Œ (ë¦¬ë””ë ‰ì…˜ ì—†ìŒ)")
            
            # ì„±ê³µ ìºì‹±
            cache_manager.set_url_redirect_cache(url, final_url, ttl=3600)  # 1ì‹œê°„
            return final_url

        except requests.exceptions.Timeout:
            logger.error(f"â° [{index}/{total}] URL ë¦¬ë””ë ‰ì…˜ íƒ€ì„ì•„ì›ƒ")
            cache_manager.set_url_redirect_cache(url, "FAILED", ttl=300)
            return None
        except Exception as e:
            logger.error(f"âŒ [{index}/{total}] URL ê²€ì¦ ì‹¤íŒ¨: {e}")
            cache_manager.set_url_redirect_cache(url, "FAILED", ttl=300)
            return None

    def extract_content_strict(self, url: str, content_parser: ContentBlockParser,
                               index: int, total: int) -> Optional[Dict]:
        """ì—„ê²©í•œ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            logger.info(f"ğŸ“„ [{index}/{total}] ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘...")

            parsed_content = content_parser.parse_url(url)

            if not parsed_content:
                logger.error(f"âŒ [{index}/{total}] íŒŒì‹± ê²°ê³¼ê°€ ì—†ìŒ")
                return None

            content_text = " ".join(
                b.content for b in parsed_content.blocks if b.type == "text"
            ).strip()

            if len(content_text) < self.min_content_length:
                logger.warning(f"âš ï¸ [{index}/{total}] ì½˜í…ì¸ ê°€ ë„ˆë¬´ ì§§ìŒ ({len(content_text)}ì)")

                try:
                    fallback = extract_news_content_with_title(url)
                    if fallback and len(fallback.get("content", "")) >= self.min_content_length:
                        content_text = fallback["content"]
                        logger.info(f"âœ… [{index}/{total}] Fallback íŒŒì„œë¡œ ë³´ì™„ ì„±ê³µ")
                    else:
                        logger.error(f"âŒ [{index}/{total}] Fallbackë„ ì‹¤íŒ¨")
                        return None
                except Exception as e:
                    logger.error(f"âŒ [{index}/{total}] Fallback íŒŒì„œ ì‹¤íŒ¨: {e}")
                    return None

            images = [block.content for block in parsed_content.blocks if block.type == 'image']

            logger.info(f"âœ… [{index}/{total}] ì½˜í…ì¸  ì¶”ì¶œ ì„±ê³µ ({len(content_text)}ì)")

            return {
                'title': parsed_content.title,
                'content_text': content_text,
                'summary_text': parsed_content.summary_text,
                'images': images,
                'main_image': images[0] if images else None,
                'parser_used': parsed_content.parser_used,
                'confidence': parsed_content.confidence,
                'blocks': parsed_content.blocks[:20]
            }

        except Exception as e:
            logger.error(f"âŒ [{index}/{total}] ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def analyze_sentiment_strict(self, url: str, index: int, total: int) -> Dict:
        """ì—„ê²©í•œ ê°ì„± ë¶„ì„ (ìºì‹± ì ìš©) - URL ê¸°ë°˜"""
        try:
            logger.info(f"ğŸ§  [{index}/{total}] FinBERT ê°ì„± ë¶„ì„ ì‹œì‘...")

            if not url or not url.startswith(('http://', 'https://')):
                logger.warning(f"âš ï¸ [{index}/{total}] ë¶„ì„í•  URLì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ")
                raise ValueError("URLì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ")

            # ìºì‹œ í™•ì¸ (URL ê¸°ì¤€)
            cached_result = cache_manager.get_sentiment_cache(url)
            if cached_result:
                logger.info(f"ğŸ¯ [{index}/{total}] FinBERT ìºì‹œ íˆíŠ¸: {cached_result.get('label')}")
                return cached_result

            # FinBERT Flask ì„œë²„ì— URL ì „ì†¡í•˜ì—¬ ë¶„ì„
            import requests
            finbert_url = "http://finbert-service:8000/analyze"
            
            payload = {"url": url}
            headers = {"Content-Type": "application/json"}
            
            response = requests.post(finbert_url, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            
            analysis_result = response.json()
            logger.info(f"âœ… [{index}/{total}] FinBERT ë¶„ì„ ì™„ë£Œ: {analysis_result.get('label')}")
            
            # ìºì‹œ ì €ì¥ (URL ê¸°ì¤€)
            cache_manager.set_sentiment_cache(url, analysis_result, ttl=86400)  # 24ì‹œê°„
            return analysis_result

        except Exception as e:
            logger.error(f"âŒ [{index}/{total}] FinBERT ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'label': 'NEUTRAL',
                'score': 0.0,
                'confidence': 0.0,
                'confidence_level': 'LOW',
                'trading_signal': 'HOLD',
                'signal_strength': 'WEAK',
                'error': str(e)
            }

    def generate_gpt_summary_strict(self, title: str, content: str,
                                   index: int, total: int) -> List[str]:
        """ì—„ê²©í•œ GPT ìš”ì•½ ìƒì„± (ìºì‹± ë° ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        try:
            # ìºì‹œ í™•ì¸
            cached_summary = cache_manager.get_summary_cache(title, content)
            if cached_summary:
                logger.info(f"ğŸ¯ [{index}/{total}] GPT ìš”ì•½ ìºì‹œ íˆíŠ¸")
                return cached_summary
            
            self.wait_for_claude_rate_limit()  # Rate limiting ìœ ì§€

            logger.info(f"ğŸ“ [{index}/{total}] GPT ìš”ì•½ ìš”ì²­ ì‹œì‘...")

            # OpenAI GPT API í˜¸ì¶œ
            import openai
            
            # API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ fallback)
            import os
            openai.api_key = os.getenv('OPENAI_API_KEY', 'your-api-key-here')
            
            if openai.api_key == 'your-api-key-here':
                logger.warning(f"âš ï¸ [{index}/{total}] OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ, Fallback ì‚¬ìš©")
                return self._generate_fallback_summary(title, content)

            # GPT í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ 3ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ì œëª©: {title[:200]}

ë‚´ìš©: {content[:1500]}

ìš”êµ¬ì‚¬í•­:
- 3ì¤„ ì´ë‚´ë¡œ ìš”ì•½
- ê° ì¤„ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±
- í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš”í•œ ì •ë³´ ìœ„ì£¼ë¡œ ìš”ì•½
- íˆ¬ì/ê²½ì œ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨"""

            # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ (ìµœëŒ€ 2íšŒ ì‹œë„)
            max_retries = 2
            for attempt in range(max_retries):
                try:
                    response = openai.ChatCompletion.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì •í™•í•œ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤."},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=200,
                        temperature=0.3,
                        timeout=30
                    )
                    
                    break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
                    
                except Exception as e:
                    if attempt < max_retries - 1:
                        logger.warning(f"â° [{index}/{total}] GPT API ì˜¤ë¥˜, ì¬ì‹œë„ {attempt+1}/{max_retries-1}: {e}")
                        time.sleep(2)  # ì¬ì‹œë„ ì „ 2ì´ˆ ëŒ€ê¸°
                        continue
                    else:
                        logger.error(f"âŒ [{index}/{total}] GPT API ì‹¤íŒ¨ (ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨): {e}")
                        return self._generate_fallback_summary(title, content)

            summary_text = response.choices[0].message.content.strip()

            if not summary_text:
                logger.warning(f"âš ï¸ [{index}/{total}] GPT ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
                return self._generate_fallback_summary(title, content)

            sentences = [
                s.strip() for s in summary_text.replace('\n', '.').split('.')
                if len(s.strip()) > 5
            ]

            result = sentences[:3] if sentences else []
            logger.info(f"âœ… [{index}/{total}] GPT ìš”ì•½ ì„±ê³µ ({len(result)}ê°œ ë¬¸ì¥)")
            
            # ìºì‹œ ì €ì¥
            cache_manager.set_summary_cache(title, content, result, ttl=604800)  # 7ì¼
            return result

        except Exception as e:
            logger.error(f"âŒ [{index}/{total}] GPT ìš”ì•½ ì‹¤íŒ¨: {e}")
            return self._generate_fallback_summary(title, content)
    
    def _generate_fallback_summary(self, title: str, content: str) -> List[str]:
        """GPT API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìš”ì•½ ìƒì„±"""
        try:
            # ì œëª©ê³¼ ì»¨í…ì¸  ì²« ë¶€ë¶„ì„ í™œìš©í•œ ê°„ë‹¨í•œ ìš”ì•½
            summary = []
            
            # ì œëª© í™œìš©
            if title:
                summary.append(title[:100])
            
            # ì»¨í…ì¸  ì²« 200ì í™œìš©
            if content and len(content) > 50:
                first_sentence = content[:200].split('.')[0]
                if first_sentence:
                    summary.append(first_sentence.strip())
            
            logger.info(f"â„¹ï¸ Fallback ìš”ì•½ ìƒì„± ì™„ë£Œ")
            return summary[:2]  # ìµœëŒ€ 2ê°œ ë¬¸ì¥ ë°˜í™˜
            
        except Exception as e:
            logger.error(f"Fallback ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return []

    def process_single_news_strict(self, news_item: NewsItem, content_parser: ContentBlockParser,
                                   index: int, total: int, collection_type: str = "REALTIME") -> Optional[Dict]:
        """ì—„ê²©í•œ ìˆœì°¨ ì²˜ë¦¬"""
        try:
            logger.info(f"ğŸ”„ [{index}/{total}] ë‰´ìŠ¤ ì²˜ë¦¬ ì‹œì‘: {news_item.title[:50]}...")
            start_time = time.time()

            # STEP 1: URL ê²€ì¦ ë° ë¦¬ë””ë ‰ì…˜ (ì‹¤íŒ¨í•˜ë©´ ìŠ¤í‚µ)
            final_url = self.verify_and_redirect_url(news_item.url, index, total)
            if not final_url:
                logger.error(f"âŒ [{index}/{total}] URL ë¦¬ë””ë ‰ì…˜ ì‹¤íŒ¨ - ì‹¤ì œ ë‰´ìŠ¤ URLì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìŠ¤í‚µ")
                return None

            # STEP 2: ì½˜í…ì¸  ì¶”ì¶œ (ì‹¤íŒ¨í•˜ë©´ ìŠ¤í‚µ)
            content_data = self.extract_content_strict(final_url, content_parser, index, total)
            if not content_data:
                logger.error(f"âŒ [{index}/{total}] ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨ - ì‹¤ì œ ë‰´ìŠ¤ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìŠ¤í‚µ")
                return None

            # STEP 3: FinBERT ê°ì„± ë¶„ì„ (ì‹¤ì œ URL ì „ì†¡)
            sentiment_result = self.analyze_sentiment_strict(final_url, index, total)

            # STEP 4: GPT ìš”ì•½ ìƒì„±
            summary_lines = self.generate_gpt_summary_strict(
                content_data['title'],
                content_data['content_text'],
                index,
                total
            )

            # STEP 5: ìµœì¢… ê²°ê³¼ êµ¬ì„±
            # ì‹¤ì œ URLê³¼ RSS URL ëª…í™•íˆ êµ¬ë¶„
            logger.debug(f"ğŸ“¦ [{index}/{total}] ìµœì¢… URL ì„¤ì • - ì‹¤ì œ: {final_url[:80]}..., RSS: {news_item.url[:80]}...")
            
            result = {
                'title': content_data['title'],
                'url': final_url,  # ì‹¤ì œ ë‰´ìŠ¤ í˜ì´ì§€ URL (ë¦¬ë””ë ‰ì…˜ í›„)
                'original_url': news_item.url,  # RSSì—ì„œ ì œê³µí•œ ì›ë³¸ URL
                'source': news_item.source,
                'published_at': news_item.published_at.isoformat(),
                'category': news_item.category,
                'symbol': news_item.symbol,
                'collection_type': collection_type,  # ìˆ˜ì§‘ íƒ€ì… ì¶”ê°€

                'content_text': content_data['content_text'][:5000],
                'summary_text': content_data['summary_text'],
                'summary_lines': summary_lines,

                'images': content_data['images'][:10],
                'main_image_url': content_data['main_image'],
                'total_images': len(content_data['images']),

                'content_blocks': [
                    {
                        'type': block.type,
                        'content': block.content[:1000] if block.type == 'text' else block.content,
                        'position': block.position,
                        'confidence': block.confidence
                    } for block in content_data['blocks']
                ],

                **sentiment_result,

                'parser_info': {
                    'parser_used': content_data['parser_used'],
                    'confidence': content_data['confidence'],
                    'processing_time': round(time.time() - start_time, 2)
                },
                'processing_status': 'success'
            }

            processing_time = time.time() - start_time
            logger.info(f"ğŸ‰ [{index}/{total}] ë‰´ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")
            return result

        except Exception as e:
            logger.error(f"âŒ [{index}/{total}] ë‰´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def process_news_batch_parallel(self, news_items: List[NewsItem], 
                                   collection_type: str = "REALTIME") -> List[Optional[Dict]]:
        """ë‰´ìŠ¤ ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬"""
        if not news_items:
            return []
        
        logger.info(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œë¡œ {len(news_items)}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ ì‹œì‘")
        
        # content_parserë¥¼ ê°€ì ¸ì™€ì„œ ì „ë‹¬
        content_parser = self.get_content_parser()
        
        # ì½˜í…ì¸  ì¶”ì¶œ í•¨ìˆ˜ë¥¼ ë˜í•‘
        def extract_content_wrapper(url, idx, total):
            return self.extract_content_strict(url, content_parser, idx, total)
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì—”ì§„ ì‚¬ìš©
        results = self.parallel_processor.process_news_batch(
            news_items,
            url_processor=self.verify_and_redirect_url,
            content_processor=extract_content_wrapper,
            sentiment_processor=self.analyze_sentiment_strict,
            summary_processor=self.generate_gpt_summary_strict
        )
        
        # collection_type ì„¤ì •
        for result in results:
            if result:
                result['collection_type'] = collection_type
        
        return results


# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
news_processor = StrictSequentialNewsProcessor()


def monitor_performance(func):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            logger.info(f"âœ… {func.__name__} ì‹¤í–‰ì‹œê°„: {duration:.2f}ì´ˆ")
            return result
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"âŒ {func.__name__} ì‹¤íŒ¨ ({duration:.2f}ì´ˆ): {e}")
            raise

    return wrapper


@app.route('/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        "status": "healthy",
        "model_loaded": is_model_loaded(),
        "thread_count": threading.active_count(),
        "processing_mode": "strict_sequential",
        "timestamp": datetime.now().isoformat()
    })


@app.route('/collect/realtime', methods=['POST'])
@monitor_performance
def collect_realtime_news():
    """ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ API"""
    try:
        data = request.get_json() or {}
        requested_limit = data.get('limit', 20)

        limit = min(requested_limit, 15)

        logger.info(f"ğŸš€ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆœì°¨ ìˆ˜ì§‘ ì‹œì‘ (ìš”ì²­: {requested_limit}, ì‹¤ì œ: {limit})")

        crawler = news_processor.get_crawler()
        content_parser = news_processor.get_content_parser()

        # RSSì—ì„œ ë‰´ìŠ¤ URL ìˆ˜ì§‘
        logger.info("ğŸ“¡ RSSì—ì„œ ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì¤‘...")
        news_items = crawler.get_realtime_news(limit)

        if not news_items:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return jsonify({
                'success': True,
                'count': 0,
                'news': [],
                'metadata': {
                    'requested': requested_limit,
                    'collected': 0,
                    'processed': 0,
                    'failed': 0,
                    'collection_type': 'REALTIME'
                }
            })

        logger.info(f"âœ… {len(news_items)}ê°œ ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì™„ë£Œ")

        # ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ í™•ì¸ (í˜„ì¬ ë¹„í™œì„±í™”)
        if False and news_processor.enable_parallel and len(news_items) >= 3:
            # ë³‘ë ¬ ì²˜ë¦¬ (3ê°œ ì´ìƒì¼ ë•Œ)
            logger.info(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ í™œì„±í™” ({len(news_items)}ê°œ)")
            results = news_processor.process_news_batch_parallel(news_items, "REALTIME")
            
            # None ì œê±° ë° í†µê³„ ê³„ì‚°
            valid_results = [r for r in results if r is not None]
            failed_count = len(results) - len(valid_results)
            results = valid_results
            
        else:
            # ìˆœì°¨ ì²˜ë¦¬ (ì†ŒëŸ‰ì¼ ë•Œ)
            logger.info(f"ğŸ“ ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ ({len(news_items)}ê°œ)")
            results = []
            failed_count = 0

            for idx, news_item in enumerate(news_items, 1):
                try:
                    logger.info(f"\n{'=' * 60}")
                    logger.info(f"â³ [{idx}/{len(news_items)}] ì²˜ë¦¬ ì¤‘...")

                    result = news_processor.process_single_news_strict(
                        news_item, content_parser, idx, len(news_items), "REALTIME"
                    )

                    if result:
                        results.append(result)
                        logger.info(f"âœ… [{idx}/{len(news_items)}] ì„±ê³µ")
                    else:
                        failed_count += 1
                        logger.warning(f"âš ï¸ [{idx}/{len(news_items)}] ì‹¤íŒ¨")

                    if idx < len(news_items):
                        time.sleep(1)

                except Exception as e:
                    failed_count += 1
                    logger.error(f"âŒ [{idx}/{len(news_items)}] ì˜ˆì™¸ ë°œìƒ: {e}")
                    continue

        success_rate = len(results) / len(news_items) * 100 if news_items else 0
        logger.info(f"ğŸ‰ ì™„ë£Œ - ì„±ê³µ: {len(results)}, ì‹¤íŒ¨: {failed_count}, ì„±ê³µë¥ : {success_rate:.1f}%")

        return jsonify({
            'success': True,
            'count': len(results),
            'news': results,
            'metadata': {
                'requested': requested_limit,
                'collected': len(news_items),
                'processed': len(results),
                'failed': failed_count,
                'success_rate': f"{success_rate:.1f}%",
                'collection_type': 'REALTIME'
            }
        })

    except Exception as e:
        logger.error(f"âŒ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return jsonify({'error': f'Failed to collect realtime news: {str(e)}'}), 500

    finally:
        try:
            news_processor.cleanup_local_resources()
            logger.info("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì˜¤ë¥˜: {e}")


@app.route('/collect/watchlist', methods=['POST'])
@monitor_performance
def collect_single_watchlist_news():
    """ë‹¨ì¼ ê´€ì‹¬ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ API (Spring Bootì—ì„œ í˜¸ì¶œ)"""
    try:
        logger.info("ğŸ“¥ ê´€ì‹¬ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ API í˜¸ì¶œë¨")
        logger.info(f"Request headers: {dict(request.headers)}")
        logger.info(f"Request data: {request.data}")
        logger.info(f"Request content type: {request.content_type}")
        
        # í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ í•´ê²° - ì—¬ëŸ¬ ì¸ì½”ë”© ë°©ì‹ ì‹œë„
        data = None
        try:
            data = request.get_json(force=True)
            logger.info(f"Parsed JSON data: {data}")
        except Exception as json_error:
            logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {json_error}")
            # ì—¬ëŸ¬ ì¸ì½”ë”© ë°©ì‹ìœ¼ë¡œ ì‹œë„
            encoding_attempts = ['utf-8', 'euc-kr', 'cp949', 'iso-8859-1']
            
            for encoding in encoding_attempts:
                try:
                    import json
                    raw_data = request.data.decode(encoding)
                    logger.info(f"Raw data with {encoding}: {raw_data}")
                    data = json.loads(raw_data)
                    logger.info(f"Successfully parsed with {encoding}: {data}")
                    break
                except Exception as enc_error:
                    logger.debug(f"ì¸ì½”ë”© {encoding} ì‹¤íŒ¨: {enc_error}")
                    continue
            
            if data is None:
                # ë§ˆì§€ë§‰ ì‹œë„: bytesë¥¼ ì§ì ‘ ì²˜ë¦¬
                try:
                    import json
                    # raw bytesë¥¼ Latin-1ë¡œ ë””ì½”ë”©í•œ í›„ ë‹¤ì‹œ UTF-8ë¡œ ì¸ì½”ë”©/ë””ì½”ë”©
                    raw_bytes = request.data
                    temp_str = raw_bytes.decode('latin-1')
                    final_str = temp_str.encode('latin-1').decode('utf-8')
                    logger.info(f"Final attempt data: {final_str}")
                    data = json.loads(final_str)
                    logger.info(f"Final attempt success: {data}")
                except Exception as final_error:
                    logger.error(f"ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨: {final_error}")
                    return jsonify({'error': f'JSON parsing failed with all encoding attempts: {str(json_error)}'}), 400
        
        if not data:
            logger.error("âŒ JSON dataê°€ ë¹„ì–´ìˆìŒ")
            return jsonify({'error': 'JSON data required'}), 400

        symbol = data.get('symbol')
        company_name = data.get('company_name')
        aliases = data.get('aliases', [])
        limit = min(data.get('limit', 5), 10)  # ì¢…ëª©ë‹¹ ìµœëŒ€ 10ê°œ

        if not symbol or not company_name:
            return jsonify({'error': 'symbol and company_name required'}), 400

        logger.info(f"ğŸš€ ê´€ì‹¬ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘: {company_name} ({symbol})")

        crawler = news_processor.get_crawler()
        content_parser = news_processor.get_content_parser()

        # Google RSSë¡œ í•´ë‹¹ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘
        news_items = crawler.get_watchlist_news(symbol, company_name, aliases, limit)

        if not news_items:
            logger.warning(f"âš ï¸ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤: {company_name}")
            return jsonify({
                'success': True,
                'symbol': symbol,
                'company_name': company_name,
                'count': 0,
                'news': []
            })

        # ìˆœì°¨ ì²˜ë¦¬
        results = []
        failed_count = 0

        for idx, news_item in enumerate(news_items, 1):
            try:
                # ì¢…ëª© ì •ë³´ ì„¤ì •
                news_item.symbol = symbol

                result = news_processor.process_single_news_strict(
                    news_item, content_parser, idx, len(news_items), "WATCHLIST"
                )

                if result:
                    # ì¢…ëª© ì •ë³´ í™•ì‹¤íˆ ì„¤ì •
                    result['symbol'] = symbol
                    result['company_name'] = company_name
                    results.append(result)
                else:
                    failed_count += 1

                if idx < len(news_items):
                    time.sleep(1)

            except Exception as e:
                failed_count += 1
                logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        logger.info(f"ğŸ‰ {company_name} ë‰´ìŠ¤ {len(results)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

        return jsonify({
            'success': True,
            'symbol': symbol,
            'company_name': company_name,
            'count': len(results),
            'failed': failed_count,
            'news': results
        })

    except Exception as e:
        logger.error(f"âŒ ê´€ì‹¬ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return jsonify({'error': f'Failed to collect watchlist news: {str(e)}'}), 500

    finally:
        try:
            news_processor.cleanup_local_resources()
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì˜¤ë¥˜: {e}")


@app.route('/collect/historical', methods=['POST'])
@monitor_performance
def collect_historical_news():
    """ì—­ì‚¬ì±Œë¦°ì§€ ë‰´ìŠ¤ ìˆ˜ì§‘ API"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'JSON data required'}), 400

        symbol = data.get('symbol')
        company_name = data.get('company_name')
        date = data.get('date')
        aliases = data.get('aliases', [])
        limit = min(data.get('limit', 10), 10)

        if not all([symbol, company_name, date]):
            return jsonify({'error': 'symbol, company_name, and date required'}), 400

        logger.info(f"ğŸš€ ì—­ì‚¬ì±Œë¦°ì§€ ë‰´ìŠ¤ ìˆ˜ì§‘: {company_name} ({date})")

        crawler = news_processor.get_crawler()
        content_parser = news_processor.get_content_parser()

        # íŠ¹ì • ë‚ ì§œ ë‰´ìŠ¤ ìˆ˜ì§‘
        news_items = crawler.get_historical_news(symbol, company_name, date, aliases, limit)

        if not news_items:
            logger.warning(f"âš ï¸ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤: {company_name} ({date})")
            return jsonify({
                'success': True,
                'symbol': symbol,
                'company_name': company_name,
                'target_date': date,
                'count': 0,
                'news': []
            })

        # ìˆœì°¨ ì²˜ë¦¬
        results = []
        failed_count = 0

        for idx, news_item in enumerate(news_items, 1):
            try:
                # ì¢…ëª© ì •ë³´ ì„¤ì •
                news_item.symbol = symbol

                result = news_processor.process_single_news_strict(
                    news_item, content_parser, idx, len(news_items), "HISTORICAL"
                )

                if result:
                    result['target_date'] = date
                    result['symbol'] = symbol
                    result['company_name'] = company_name
                    results.append(result)
                else:
                    failed_count += 1

                if idx < len(news_items):
                    time.sleep(1)

            except Exception as e:
                failed_count += 1
                logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        logger.info(f"ğŸ‰ ì—­ì‚¬ì±Œë¦°ì§€ ë‰´ìŠ¤ {len(results)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

        return jsonify({
            'success': True,
            'symbol': symbol,
            'company_name': company_name,
            'target_date': date,
            'count': len(results),
            'failed': failed_count,
            'news': results
        })

    except Exception as e:
        logger.error(f"âŒ ì—­ì‚¬ì±Œë¦°ì§€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return jsonify({'error': f'Failed to collect historical news: {str(e)}'}), 500

    finally:
        try:
            news_processor.cleanup_local_resources()
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì˜¤ë¥˜: {e}")


@app.route('/stats', methods=['GET'])
def get_collection_stats():
    """ìˆ˜ì§‘ í†µê³„ ì¡°íšŒ"""
    return jsonify({
        'stats': news_processor.collection_stats,
        'cache_stats': cache_manager.get_stats(),  # ìºì‹œ í†µê³„ ì¶”ê°€
        'timestamp': datetime.now().isoformat()
    })

@app.route('/cache/stats', methods=['GET'])
def get_cache_stats():
    """ìºì‹œ í†µê³„ ì¡°íšŒ API"""
    return jsonify(cache_manager.get_stats())

@app.route('/cache/clear', methods=['POST'])
def clear_cache():
    """ìºì‹œ ì‚­ì œ API"""
    pattern = request.args.get('pattern', None)
    cache_manager.clear_cache(pattern)
    return jsonify({'message': 'Cache cleared', 'pattern': pattern})


# ê¸°ì¡´ ë¶„ì„ APIë“¤ ìœ ì§€
@app.route('/analyze', methods=['POST'])
@monitor_performance
def analyze_sentiment_endpoint():
    """ë‰´ìŠ¤ URL ê¸°ë°˜ ê°ì •ë¶„ì„ API"""
    try:
        if not is_model_loaded():
            return jsonify({"error": "Model not loaded"}), 500

        if not request.is_json:
            return jsonify({"error": "Content-Type must be application/json"}), 400

        data = request.get_json()
        url = data.get('url', '').strip()

        if not url:
            return jsonify({"error": "url field is required"}), 400

        if not url.startswith(('http://', 'https://')):
            return jsonify({"error": "Invalid URL format"}), 400

        logger.info(f"ë‰´ìŠ¤ ë¶„ì„ ìš”ì²­: {url}")

        extracted_data = extract_news_content_with_title(url)
        combined_text = extracted_data['combined_text']

        if len(combined_text) > TEXT_MAX_LENGTH:
            combined_text = combined_text[:TEXT_MAX_LENGTH]

        analysis_result = analyze_sentiment(combined_text)

        result = {
            "url": url,
            "title": extracted_data['title'],
            "content": extracted_data['content'],
            "images": extracted_data['images'],
            **analysis_result
        }

        return jsonify(result)

    except Exception as e:
        logger.error(f"ê°ì •ë¶„ì„ ì˜¤ë¥˜: {e}")
        return jsonify({"error": f"Internal server error: {str(e)}"}), 500


@app.route('/analyze-text', methods=['POST'])
@monitor_performance
def analyze_text_sentiment_endpoint():
    """í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ ê¸°ë°˜ ê°ì •ë¶„ì„ API"""
    try:
        if not is_model_loaded():
            return jsonify({"error": "Model not loaded"}), 500

        if not request.is_json:
            return jsonify({"error": "Content-Type must be application/json"}), 400

        data = request.get_json()
        text = data.get('text', '').strip()

        if not text:
            return jsonify({"error": "text field is required"}), 400

        logger.info(f"í…ìŠ¤íŠ¸ ë¶„ì„ ìš”ì²­: {len(text)}ì")

        if len(text) > TEXT_MAX_LENGTH:
            text = text[:TEXT_MAX_LENGTH]

        analysis_result = analyze_sentiment(text)

        return jsonify(analysis_result)

    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ê°ì •ë¶„ì„ ì˜¤ë¥˜: {e}")
        return jsonify({"error": f"Internal server error: {str(e)}"}), 500


if __name__ == '__main__':
    logger.info("Flask ì„œë²„ ì‹œì‘ ì¤‘...")

    if not load_model():
        logger.error("ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë¡œ ì¸í•´ ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        exit(1)

    logger.info("ì„œë²„ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ!")

    app.run(host=HOST, port=PORT, debug=DEBUG, threaded=True)